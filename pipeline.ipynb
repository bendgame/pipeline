{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import category_encoders as ce\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_features= PolynomialFeatures(degree=2, interaction_only=True)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble \n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "from features.features import get_year, get_word_count, get_encoded_text\n",
    "from features.features import get_numeric_data ,get_combine_text ,get_reset_index,get_text_data \n",
    "from features.features import SparseInteractions\n",
    "#year_column= 'title'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>rating</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>title</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This wine is light in tannins and ripe in frui...</td>\n",
       "      <td>85</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>Quinta dos Avidagos 2015 Lote 138 Red (Douro)</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country                                        description  rating  price  \\\n",
       "0  Portugal  This is ripe and fruity, a wine that is smooth...      87   15.0   \n",
       "1  Portugal  This wine is light in tannins and ripe in frui...      85   11.0   \n",
       "\n",
       "  province                                          title               winery  \n",
       "0    Douro  Quinta dos Avidagos 2011 Avidagos Red (Douro)  Quinta dos Avidagos  \n",
       "1    Douro  Quinta dos Avidagos 2015 Lote 138 Red (Douro)  Quinta dos Avidagos  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect(r'C:\\Users\\MTGro\\Desktop\\coding\\wineApp\\db\\wine_data.sqlite')\n",
    "c = conn.cursor\n",
    "\n",
    "df = pd.read_sql(\"select country  \\\n",
    "                 ,description     \\\n",
    "                 ,rating          \\\n",
    "                 ,price           \\\n",
    "                 ,province        \\\n",
    "                 ,title           \\\n",
    "                 ,winery from wine_data limit 70000\", conn)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sklearn.feature_extraction.text\n",
    "cvec = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "hvec = HashingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Token Patterns\n",
    "TOKENS_ALPHANUMERIC_d ='(?u)\\\\b\\\\w\\\\w+\\\\b' # Default\n",
    "TOKENS_ALPHANUMERIC_1 = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "TOKENS_ALPHANUMERIC_2 = r'\\w{1,}'\n",
    "\n",
    "### arguments applicable to all text vectorizers\n",
    "base_args = {\n",
    "        'encoding' : 'utf-8'\n",
    "        ,'decode_error' : 'strict'\n",
    "        ,'strip_accents' : None\n",
    "        ,'lowercase' : True\n",
    "        ,'preprocessor': None\n",
    "        ,'tokenizer' : None\n",
    "        ,'analyzer' : 'word'\n",
    "        ,'stop_words': None\n",
    "        ,'token_pattern' : TOKENS_ALPHANUMERIC_d\n",
    "        ,'ngram_range' : (1,1)\n",
    "    }\n",
    "\n",
    "### specific to Count Vectorizer\n",
    "cvec_args = {\n",
    "        'max_df' : 1.0\n",
    "        ,'min_df' : 1\n",
    "        ,'max_features' : None\n",
    "        ,'vocabulary': None\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "### specific to Tfidf Vectorizer\n",
    "tfidf_args = {\n",
    "        'max_df' : 1.0\n",
    "        ,'min_df' : 1\n",
    "        ,'max_features' : None\n",
    "        ,'vocabulary' : None\n",
    "        ,'use_idf' : True\n",
    "        ,'smooth_idf' : True\n",
    "        ,'sublinear_tf' : False\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "### specific to Hashing Vectorizer\n",
    "hashing_args = {\n",
    "        'n_features' : 1048576\n",
    "        ,'norm' : 'l2'\n",
    "        ,'alternate_sign' : True\n",
    "    }\n",
    "\n",
    "\n",
    "### Merge contents of dict2 and dict1 to dict3\n",
    "cvec_params = {**base_args , **cvec_args}\n",
    "tfidf_params = {**base_args , **tfidf_args}\n",
    "hashing_params = {**base_args , **hashing_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HashingVectorizer(alternate_sign=True, analyzer='word', binary=False,\n",
       "                  decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "                  encoding='utf-8', input='content', lowercase=True,\n",
       "                  n_features=1048576, ngram_range=(1, 1), norm='l2',\n",
       "                  preprocessor=None, stop_words=None, strip_accents=None,\n",
       "                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the parameters for the text vectorizers\n",
    "cvec.set_params(**cvec_params)\n",
    "tfidf.set_params(**tfidf_params)\n",
    "hvec.set_params(**hashing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct the Text Featrues Pipeline\n",
    "\n",
    "## Set feature number\n",
    "chi_k = 12\n",
    "\n",
    "text_features = Pipeline([\n",
    "                    ('reset_index', get_reset_index),\n",
    "                    ('combine_text',get_combine_text),\n",
    "                    ('selector', get_text_data),\n",
    "                    ('t_vectorizer', tfidf),\n",
    "#                     ('c_vectorizer', cvec),\n",
    "#                     ('h_vectorizer', hvec),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = Pipeline([\n",
    "                    ('reset_index', get_reset_index),\n",
    "                    ('year', get_year),\n",
    "                    ('word_count', get_word_count),\n",
    "                    ('encode', get_encoded_text),\n",
    "                    ('selector', get_numeric_data),\n",
    "                    #('features', polynomial_features),\n",
    "                    #('imputer', SimpleImputer(strategy='constant', fill_value='missing'))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine the text features with the numeric ceatures\n",
    "preprocessor = Pipeline(steps =[('union', FeatureUnion(\n",
    "                transformer_list = [\n",
    "                    ('numeric_features', numeric_features),\n",
    "                    ('text_features', text_features)\n",
    "                ]))  \n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "        NuSVC(probability=True),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        AdaBoostClassifier(),\n",
    "        GradientBoostingClassifier()\n",
    "    ]\n",
    "    \n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', classifier)])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)   \n",
    "    \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GradientBoostingClassifier(\n",
    "        criterion = 'friedman_mse'\n",
    "        ,init = None\n",
    "        ,learning_rate = 0.1\n",
    "        ,loss = 'deviance'\n",
    "        ,max_depth = 3\n",
    "        ,max_features = None\n",
    "        ,max_leaf_nodes = None\n",
    "        ,min_impurity_decrease = 0.0\n",
    "        ,min_impurity_split = None\n",
    "        ,min_samples_leaf = 1\n",
    "        ,min_samples_split = 2\n",
    "        ,min_weight_fraction_leaf = 0.0\n",
    "        ,n_estimators = 100\n",
    "        ,n_iter_no_change = None\n",
    "        ,presort = 'auto'\n",
    "        ,random_state = None\n",
    "        ,subsample = 1.0\n",
    "        ,tol = 0.0001\n",
    "        ,validation_fraction = 0.1\n",
    "        ,verbose = 0\n",
    "        ,warm_start = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   4.5s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total= 2.5min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25357142857142856"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                  ('classifier', classifier)], verbose = True)\n",
    "\n",
    "# loss=’ls’, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=’friedman_mse’\n",
    "# , min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "# min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None\n",
    "# , alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort=’auto’, validation_fraction=0.1\n",
    "# , n_iter_no_change=None, tol=0.0001\n",
    "\n",
    "# param_grid = { \n",
    "#     'classifier__n_estimators': [3000],\n",
    "#     'classifier__max_features': [ 'sqrt'],\n",
    "#     'classifier__max_depth' : [7,8,9],\n",
    "#     'classifier__learning_rate': [0.02,0.025,0.029],\n",
    "#     'classifier__min_samples_split': [27],\n",
    "#     'classifier__min_samples_leaf': [21], \n",
    "#     'classifier__loss' :['huber']\n",
    "# }\n",
    "\n",
    "features = df.drop(['rating'], axis=1)\n",
    "\n",
    "X = features\n",
    "y = df['rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y\n",
    "                                                   , test_size = .3\n",
    "                                                   #, stratify=y\n",
    "                                                   , random_state = 42\n",
    "                                                   , shuffle=True\n",
    "                                                   )\n",
    "\n",
    "\n",
    "# CV = GridSearchCV(pl, param_grid, n_jobs= -1)\n",
    "                  \n",
    "# CV.fit(X_train, y_train)  \n",
    "# print(CV.best_params_)    \n",
    "# print(CV.best_score_)\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "pl.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Tests\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total= 1.3min\n",
    "# {'classifier__learning_rate': 0.02, 'classifier__loss': 'huber', 'classifier__max_depth': 3, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 15, 'classifier__min_samples_split': 27, 'classifier__n_estimators': 3000}\n",
    "# 0.6289488555219046\n",
    "\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total= 2.1min\n",
    "# {'classifier__learning_rate': 0.02, 'classifier__loss': 'huber', 'classifier__max_depth': 7\n",
    "# , 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 21, 'classifier__min_samples_split': 27\n",
    "#             , 'classifier__n_estimators': 3000}\n",
    "# 0.6299899436258168\n",
    "\n",
    "\n",
    "\n",
    "#original Tests\n",
    "##TEST 1\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total=  10.4s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 7, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 500}\n",
    "# 0.5591003524136465\n",
    "\n",
    "##TEST 2\n",
    "#[Pipeline] ........ (step 2 of 2) Processing classifier, total=   9.6s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 13, 'classifier__min_samples_split': 13, 'classifier__n_estimators': 500}\n",
    "# 0.5611780620229795\n",
    "\n",
    "##TEST 3\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total=   9.5s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 13, 'classifier__min_samples_split': 13, 'classifier__n_estimators': 500}\n",
    "# 0.5615370173848946\n",
    "\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total=  14.4s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 6, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 13, 'classifier__min_samples_split': 27, 'classifier__n_estimators': 600}\n",
    "# 0.5625332421263227\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(X_train, y_train)\n",
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
