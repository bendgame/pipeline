{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import category_encoders as ce\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import category_encoders as ce\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_features= PolynomialFeatures(degree=2, interaction_only=True)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble \n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "# sys.path.append(src_dir)\n",
    "\n",
    "# from features.features import get_year, get_word_count, get_encoded_text\n",
    "# from features.features import get_numeric_data ,get_combine_text ,get_reset_index,get_text_data \n",
    "# from features.features import SparseInteractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>rating</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>title</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This wine is light in tannins and ripe in frui...</td>\n",
       "      <td>85</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>Quinta dos Avidagos 2015 Lote 138 Red (Douro)</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country                                        description  rating  price  \\\n",
       "0  Portugal  This is ripe and fruity, a wine that is smooth...      87   15.0   \n",
       "1  Portugal  This wine is light in tannins and ripe in frui...      85   11.0   \n",
       "\n",
       "  province                                          title               winery  \n",
       "0    Douro  Quinta dos Avidagos 2011 Avidagos Red (Douro)  Quinta dos Avidagos  \n",
       "1    Douro  Quinta dos Avidagos 2015 Lote 138 Red (Douro)  Quinta dos Avidagos  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the dataframe as df from the data source\n",
    "\n",
    "# df = pd.read_csv()\n",
    "# df = pd.read_excel()\n",
    "\n",
    "conn = sqlite3.connect(r'C:\\Users\\MTGro\\Desktop\\coding\\wineApp\\db\\wine_data.sqlite')\n",
    "c = conn.cursor\n",
    "\n",
    "df = pd.read_sql(\"select country  \\\n",
    "                 ,description     \\\n",
    "                 ,rating          \\\n",
    "                 ,price           \\\n",
    "                 ,province        \\\n",
    "                 ,title           \\\n",
    "                 ,winery from wine_data limit 20000\", conn)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions for the Pipeline\n",
    "The functions take a dataframe and return a dataframe so they can easily be used in the pipeline as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Reset Index of Dataframe\n",
    "Useful if you need to loop starting at index 0 post train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(dataframe):\n",
    "    dataframe = dataframe.reset_index(inplace = False)\n",
    "    return dataframe\n",
    "\n",
    "#Create Transformer for pipeline\n",
    "get_reset_index = FunctionTransformer(reset_index, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Count the number of words in a column\n",
    "Creates a new column named word_count\n",
    "\n",
    "Specify the column for which you want a word count using the global variable word_count_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = 'description'\n",
    "punc = [';'\n",
    "        , \"'\"\n",
    "        , '--'\n",
    "        , ':'\n",
    "        , '\"'\n",
    "        , \"!\"\n",
    "        , \"?\"\n",
    "        , '-'\n",
    "        , ','\n",
    "        , '.'\n",
    "        , \"(\"\n",
    "        , \")\"\n",
    "        , '$'\n",
    "        ,'`'\n",
    "        ,'~'\n",
    "       ]\n",
    "        \n",
    "        \n",
    "        \n",
    "def features(dataframe) :\n",
    "    dataframe['word_count'] = dataframe[text_column].apply(lambda x : len(x.split()))\n",
    "    dataframe['char_count'] = dataframe[text_column].apply(lambda x : len(x.replace(\" \",\"\")))\n",
    "    dataframe['word_density'] = dataframe['word_count'] / (dataframe['char_count'] + 1)\n",
    "    dataframe['punc_count'] = dataframe[text_column].apply(lambda x : len([a for a in x if a in punc]))\n",
    "    dataframe['total_length'] = dataframe[text_column].apply(len)\n",
    "    dataframe['capitals'] = dataframe[text_column].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "    dataframe['caps_vs_length'] = dataframe.apply(lambda row: float(row['capitals'])/float(row['total_length']),axis=1)\n",
    "    dataframe['num_exclamation_marks'] =dataframe[text_column].apply(lambda x: x.count('!'))\n",
    "    dataframe['num_question_marks'] = dataframe[text_column].apply(lambda x: x.count('?'))\n",
    "    dataframe['num_punctuation'] = dataframe[text_column].apply(lambda x: sum(x.count(w) for w in '.,;:'))\n",
    "    dataframe['num_symbols'] = dataframe[text_column].apply(lambda x: sum(x.count(w) for w in '*&$%'))\n",
    "    dataframe['num_unique_words'] = dataframe[text_column].apply(lambda x: len(set(w for w in x.split())))\n",
    "    dataframe['words_vs_unique'] = dataframe['num_unique_words'] / dataframe['word_count']\n",
    "    dataframe[\"word_unique_percent\"] =  dataframe[\"num_unique_words\"]*100/dataframe['word_count']\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "#Create Transformer for pipeline\n",
    "get_word_count = FunctionTransformer(features, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Combine Text Columns\n",
    "Creates a new column named text\n",
    "\n",
    "Specifiy the columns you want to combine in the global variable combine_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_text = ['country','province','title','winery', 'description']\n",
    "\n",
    "def combine_text_columns(dataframe):\n",
    "    global combine_text\n",
    "    text_data = dataframe[combine_text]\n",
    "    \n",
    "    # replace nans with blanks\n",
    "    text_data.fillna(\"\", inplace=True)\n",
    "    \n",
    "    # joins all of the text items in a row (axis=1)\n",
    "    # with a space in between\n",
    "    dataframe['text'] = text_data.apply(lambda x: \" \".join(x), axis=1)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "#Create Transformer for pipeline using the combine_text_columns function\n",
    "get_combine_text = FunctionTransformer(combine_text_columns, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Category Encoder\n",
    "pip install category_encoders / conda install -c conda-forge category_encoders\n",
    "\n",
    "Specify the columns you want to encode in the variable category_columns\n",
    "\n",
    "Specify the target column using the variable category_target if needed\n",
    "\n",
    "Adjust the encoder as needed.\n",
    "\n",
    "Takes in a dataframe and outputs a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = ['country','province','title','winery']\n",
    "category_target = 'price'\n",
    "\n",
    "# ce.BackwardDifferenceEncoder(cols=[...])\n",
    "# ce.BaseNEncoder(cols=[...])\n",
    "# ce.BinaryEncoder(cols=[...])\n",
    "# ce.CatBoostEncoder(cols=[...])\n",
    "# ce.HashingEncoder(cols=[...])\n",
    "# ce.HelmertEncoder(cols=[...])\n",
    "# ce.JamesSteinEncoder(cols=[...])\n",
    "# ce.LeaveOneOutEncoder(cols=[...])\n",
    "# ce.MEstimateEncoder(cols=[...])\n",
    "# ce.OneHotEncoder(cols=[...])\n",
    "# ce.OrdinalEncoder(cols=[...])\n",
    "# ce.SumEncoder(cols=[...])\n",
    "# ce.PolynomialEncoder(cols=[...])\n",
    "# ce.TargetEncoder(cols=[...])\n",
    "# ce.WOEEncoder(cols=[...])\n",
    "\n",
    "\n",
    "def category_encode(dataframe):\n",
    "    global category_columns\n",
    "    global category_target\n",
    "    x = dataframe[category_columns]\n",
    "    y = dataframe[category_target]\n",
    "    ce_ord = ce.JamesSteinEncoder(cols=category_columns)\n",
    "    dataframe[category_columns] = ce_ord.fit_transform(x, y)\n",
    "    return dataframe\n",
    "\n",
    "#Create Transformer for pipeline\n",
    "get_encoded_text = FunctionTransformer(category_encode, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Text Vectorizer\n",
    "\n",
    "### The following vectorizers can be configured: \n",
    "\n",
    "##### Count Vectorizer\n",
    "\n",
    "##### TF-IDF vectorizer  \n",
    "\n",
    "##### Hashing Vectorizer \n",
    "\n",
    "Modify the base_args and the specific args. \n",
    "The parameters are passed in to the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sklearn.feature_extraction.text\n",
    "cvec = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "hvec = HashingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Token Patterns\n",
    "TOKENS_ALPHANUMERIC_d ='(?u)\\\\b\\\\w\\\\w+\\\\b' # Default\n",
    "TOKENS_ALPHANUMERIC_1 = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "TOKENS_ALPHANUMERIC_2 = r'\\w{1,}'\n",
    "\n",
    "### arguments applicable to all text vectorizers\n",
    "base_args = {\n",
    "        'encoding' : 'utf-8'\n",
    "        ,'decode_error' : 'strict'\n",
    "        ,'strip_accents' : None\n",
    "        ,'lowercase' : True\n",
    "        ,'preprocessor': None\n",
    "        ,'tokenizer' : None\n",
    "        ,'analyzer' : 'word'\n",
    "        ,'stop_words': None\n",
    "        ,'token_pattern' : TOKENS_ALPHANUMERIC_d\n",
    "        ,'ngram_range' : (1,1)\n",
    "    }\n",
    "\n",
    "### specific to Count Vectorizer\n",
    "cvec_args = {\n",
    "        'max_df' : 1.0\n",
    "        ,'min_df' : 1\n",
    "        ,'max_features' : None\n",
    "        ,'vocabulary': None\n",
    "    }\n",
    "\n",
    "### specific to Tfidf Vectorizer\n",
    "tfidf_args = {\n",
    "        'max_df' : 1.0\n",
    "        ,'min_df' : 1\n",
    "        ,'max_features' : None\n",
    "        ,'vocabulary' : None\n",
    "        ,'use_idf' : True\n",
    "        ,'smooth_idf' : True\n",
    "        ,'sublinear_tf' : False\n",
    "    }\n",
    "\n",
    "### specific to Hashing Vectorizer\n",
    "hashing_args = {\n",
    "        'n_features' : 1048576\n",
    "        ,'norm' : 'l2'\n",
    "        ,'alternate_sign' : True\n",
    "    }\n",
    "\n",
    "### Merge contents of dict2 and dict1 to dict3\n",
    "cvec_params = {**base_args , **cvec_args}\n",
    "tfidf_params = {**base_args , **tfidf_args}\n",
    "hashing_params = {**base_args , **hashing_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#set the parameters for the text vectorizers\n",
    "cvec.set_params(**cvec_params)\n",
    "tfidf.set_params(**tfidf_params)\n",
    "hvec.set_params(**hashing_params)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Two select k best options:\n",
    "\n",
    "chi2\n",
    "\n",
    "f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_k = 300\n",
    "fs__kbest = SelectKBest(chi2, chi_k)\n",
    "fs__f_classif = SelectKBest(f_classif, chi_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the text_features pipeline\n",
    "Use variable text to declare text columns\n",
    "\n",
    "Use comments to adjust vectorizer as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare which columns to will be unioned with the numeric features.\n",
    "text = 'text'\n",
    "get_text_data = FunctionTransformer(lambda x: x[text], validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct a pipeline \n",
    "text_features = Pipeline([\n",
    "                    ('reset_index', get_reset_index),\n",
    "                    ('combine_text',get_combine_text),\n",
    "                    ('selector', get_text_data),\n",
    "                    ('t_vectorizer', tfidf),\n",
    "#                     ('c_vectorizer', cvec),\n",
    "#                     ('h_vectorizer', hvec),\n",
    "                    ('feature_selection', fs__kbest)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the numeric_features pipeline\n",
    "Use variable numeric to list numeric columns\n",
    "\n",
    "Use comments to adjust vectorizer as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the columns that will be encoded \n",
    "numeric= ['price', 'country','province','title','winery'\n",
    "          ,'word_count'\n",
    "          ,'char_count'\n",
    "          ,'word_density'\n",
    "          ,'punc_count'\n",
    "          ,'total_length'\n",
    "          ,'capitals'\n",
    "          ,'caps_vs_length'\n",
    "          ,'num_exclamation_marks'\n",
    "          ,'num_question_marks'\n",
    "          ,'num_punctuation'\n",
    "          ,'num_symbols'\n",
    "          ,'num_unique_words'\n",
    "          ,'words_vs_unique'\n",
    "          ,'word_unique_percent'\n",
    "         ]\n",
    "\n",
    "#transformer used to select the numeric columns to union with text features\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[numeric], validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passes the dataframe through the set of transformers\n",
    "#encodes the data\n",
    "#selects the numeric columns\n",
    "\n",
    "numeric_features = Pipeline([\n",
    "                    ('reset_index', get_reset_index),\n",
    "                    ('word_count', get_word_count),\n",
    "                    ('encode_categories', get_encoded_text),\n",
    "                    ('selector', get_numeric_data),\n",
    "#                     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#                     ('features', polynomial_features)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine the text features with the numeric ceatures\n",
    "union_features = Pipeline(steps =[('union', FeatureUnion(\n",
    "                transformer_list = [\n",
    "                    ('numeric_features', numeric_features),\n",
    "                    ('text_features', text_features)\n",
    "                ]))  \n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "        LogisticRegression(C=1.0),\n",
    "        MultinomialNB(),\n",
    "        KNeighborsClassifier(20),\n",
    "        #SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "        #NuSVC(probability=True),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        AdaBoostClassifier(),\n",
    "        GradientBoostingClassifier()\n",
    "        ]\n",
    "\n",
    "features = df.drop(['rating'], axis=1)\n",
    "\n",
    "X = features\n",
    "y = df['rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y\n",
    "                                                   , test_size = .3\n",
    "                                                   #, stratify=y\n",
    "                                                   , random_state = 42\n",
    "                                                   , shuffle=True\n",
    "                                                   )    \n",
    "\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pl = Pipeline(steps=[('preprocessor', union_features),\n",
    "                      ('classifier', classifier)])\n",
    "    \n",
    "    pl.fit(X_train, y_train)   \n",
    "    \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GradientBoostingClassifier()\n",
    "param_grid = {\n",
    "        'classifier__criterion' : ['friedman_mse'] #friedman_mse, mae, mse\n",
    "        ,'classifier__n_estimators' : [3000]\n",
    "        ,'classifier__max_depth' : [7]\n",
    "        ,'classifier__learning_rate': [0.025,0.09]\n",
    "        ,'classifier__random_state' : [42]\n",
    "        ,'classifier__min_samples_leaf' : [21]\n",
    "        ,'classifier__min_samples_split' : [27]\n",
    "        ,'classifier__loss' : ['deviance']  #‘deviance’, ‘exponential’\n",
    "#         ,'classifier__init' : [None]\n",
    "        ,'classifier__max_features' : ['sqrt'] #None, 'auto', 'sqrt', 'log2'\n",
    "#         ,'classifier__max_leaf_nodes': [None]\n",
    "#         ,'classifier__min_impurity_decrease' : [0.0]\n",
    "#         ,'classifier__min_impurity_split' : [None]\n",
    "#         ,'classifier__min_weight_fraction_leaf' : [0.0]\n",
    "#         ,'classifier__n_iter_no_change' : [None]\n",
    "#         ,'classifier__subsample' : [1.0]\n",
    "#         ,'classifier__tol' : [0.0001]\n",
    "#         ,'classifier__validation_fraction' : [0.1]\n",
    "#         ,'classifier__verbose' : [0]\n",
    "#         ,'classifier__warm_start' : [False]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline(steps=[('preprocessor', union_features),\n",
    "                  ('classifier', classifier)], verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   2.7s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=14.3min\n",
      "{'classifier__criterion': 'friedman_mse', 'classifier__learning_rate': 0.025, 'classifier__loss': 'deviance', 'classifier__max_depth': 7, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 21, 'classifier__min_samples_split': 27, 'classifier__n_estimators': 3000, 'classifier__random_state': 42}\n",
      "0.19707142857142856\n"
     ]
    }
   ],
   "source": [
    "target = 'rating'\n",
    "features = df.drop([target], axis=1)\n",
    "\n",
    "X = features\n",
    "y = df[target]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y\n",
    "                                                   , test_size = .3\n",
    "                                                   #, stratify = y\n",
    "                                                   , random_state = 42\n",
    "                                                   , shuffle=True\n",
    "                                                   )\n",
    "\n",
    "\n",
    "CV = GridSearchCV(pl, param_grid, n_jobs= -1)\n",
    "                  \n",
    "CV.fit(X_train, y_train)  \n",
    "print(CV.best_params_)    \n",
    "print(CV.best_score_)\n",
    "\n",
    "# pl.fit(X_train, y_train)\n",
    "# pl.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Tests\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total= 1.3min\n",
    "# {'classifier__learning_rate': 0.02, 'classifier__loss': 'huber', 'classifier__max_depth': 3, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 15, 'classifier__min_samples_split': 27, 'classifier__n_estimators': 3000}\n",
    "# 0.6289488555219046\n",
    "\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total= 2.1min\n",
    "# {'classifier__learning_rate': 0.02, 'classifier__loss': 'huber', 'classifier__max_depth': 7\n",
    "# , 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 21, 'classifier__min_samples_split': 27\n",
    "#             , 'classifier__n_estimators': 3000}\n",
    "# 0.6299899436258168\n",
    "\n",
    "\n",
    "\n",
    "#original Tests\n",
    "##TEST 1\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total=  10.4s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 7, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 500}\n",
    "# 0.5591003524136465\n",
    "\n",
    "##TEST 2\n",
    "#[Pipeline] ........ (step 2 of 2) Processing classifier, total=   9.6s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 13, 'classifier__min_samples_split': 13, 'classifier__n_estimators': 500}\n",
    "# 0.5611780620229795\n",
    "\n",
    "##TEST 3\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total=   9.5s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 13, 'classifier__min_samples_split': 13, 'classifier__n_estimators': 500}\n",
    "# 0.5615370173848946\n",
    "\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total=  14.4s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 6, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 13, 'classifier__min_samples_split': 27, 'classifier__n_estimators': 600}\n",
    "# 0.5625332421263227\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(X_train, y_train)\n",
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test, clf.predict_proba(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
