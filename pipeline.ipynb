{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import category_encoders as ce\n",
    "import re\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_features= PolynomialFeatures(degree=2, interaction_only=True)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble \n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "from features.features import get_year, get_word_count, get_encoded_text\n",
    "from features.features import get_numeric_data ,get_combine_text ,get_reset_index,get_text_data \n",
    "from features.features import SparseInteractions\n",
    "#year_column= 'title'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>rating</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>title</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This wine is light in tannins and ripe in frui...</td>\n",
       "      <td>85</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>Quinta dos Avidagos 2015 Lote 138 Red (Douro)</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country                                        description  rating  price  \\\n",
       "0  Portugal  This is ripe and fruity, a wine that is smooth...      87   15.0   \n",
       "1  Portugal  This wine is light in tannins and ripe in frui...      85   11.0   \n",
       "\n",
       "  province                                          title               winery  \n",
       "0    Douro  Quinta dos Avidagos 2011 Avidagos Red (Douro)  Quinta dos Avidagos  \n",
       "1    Douro  Quinta dos Avidagos 2015 Lote 138 Red (Douro)  Quinta dos Avidagos  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect(r'C:\\Users\\MTGro\\Desktop\\coding\\wineApp\\db\\wine_data.sqlite')\n",
    "c = conn.cursor\n",
    "\n",
    "df = pd.read_sql(\"select country  \\\n",
    "                 ,description     \\\n",
    "                 ,rating          \\\n",
    "                 ,price           \\\n",
    "                 ,province        \\\n",
    "                 ,title           \\\n",
    "                 ,winery from wine_data limit 70000\", conn)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = ensemble.GradientBoostingRegressor(\n",
    "    n_estimators = 600, #how many decision trees to build\n",
    "    learning_rate = 0.01, #controls rate at which additional decision trees influes overall prediction\n",
    "    max_depth = 6, \n",
    "    min_samples_split = 27,\n",
    "    min_samples_leaf = 13, \n",
    "    #max_features = 0.5,\n",
    "    loss = 'huber'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TOKENS_ALPHANUMERIC_1 = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "TOKENS_ALPHANUMERIC_2 = r'\\w{1,}'\n",
    "TOKENS_ALPHANUMERIC_3 ='(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
    "\n",
    "\n",
    "chi_k = 300\n",
    "\n",
    "numeric_features = Pipeline([\n",
    "                    ('reset_index', get_reset_index),\n",
    "                    ('year', get_year),\n",
    "                    ('word_count', get_word_count),\n",
    "                    ('encode', get_encoded_text),\n",
    "                    ('selector', get_numeric_data),\n",
    "                    #('features', polynomial_features),\n",
    "                    #('imputer', Imputer())\n",
    "                ])\n",
    "\n",
    "\n",
    "text_features = Pipeline([\n",
    "                    ('reset_index', get_reset_index),\n",
    "                    ('combine_text',get_combine_text),\n",
    "                    ('selector', get_text_data),\n",
    "                    ('t_vectorizer', TfidfVectorizer(\n",
    "                                      #min_df=3\n",
    "#                                     , max_features=None \n",
    "#                                     , strip_accents='unicode'\n",
    "#                                     , analyzer='word'\n",
    "#                                     , token_pattern= TOKENS_ALPHANUMERIC_1\n",
    "#                                     , ngram_range=(1, 1)\n",
    "#                                     , use_idf=1\n",
    "#                                     , smooth_idf=1\n",
    "#                                     , sublinear_tf=1\n",
    "#                                     , stop_words = 'english'\n",
    "                                    )),\n",
    "                    \n",
    "#                     ('c_vectorizer', CountVectorizer(token_pattern = TOKENS_ALPHANUMERIC_1\n",
    "#                                     ,ngram_range=(1, 3)\n",
    "#                                     #, stop_words = 'english'\n",
    "#                                     )),\n",
    "    \n",
    "#                     ('h_vectorizer', HashingVectorizer(token_pattern = TOKENS_ALPHANUMERIC_1,\n",
    "#                                      norm=None, binary=True,\n",
    "#                                      ngram_range=(1, 2)\n",
    "#                                      #, stop_words = 'english'\n",
    "#                                     )),    \n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ])\n",
    "\n",
    "preprocessor = Pipeline(steps =[\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', numeric_features),\n",
    "                ('text_features', text_features)\n",
    "                ]))  \n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   4.9s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total= 1.3min\n",
      "{'classifier__learning_rate': 0.02, 'classifier__loss': 'huber', 'classifier__max_depth': 3, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 15, 'classifier__min_samples_split': 27, 'classifier__n_estimators': 3000}\n",
      "0.6289488555219046\n"
     ]
    }
   ],
   "source": [
    "pl = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                  ('classifier', classifier)], verbose = True)\n",
    "\n",
    "# loss=’ls’, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=’friedman_mse’\n",
    "# , min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "# min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None\n",
    "# , alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort=’auto’, validation_fraction=0.1\n",
    "# , n_iter_no_change=None, tol=0.0001\n",
    "\n",
    "param_grid = { \n",
    "    'classifier__n_estimators': [400, 500, 600,3000],\n",
    "    'classifier__max_features': ['auto', 'sqrt', None],\n",
    "    'classifier__max_depth' : [1,2,3],\n",
    "    'classifier__learning_rate': [0.01,0.02,0.03], \n",
    "    'classifier__min_samples_split': [27,91],\n",
    "    'classifier__min_samples_leaf': [13,15], \n",
    "    'classifier__loss' :['ls', 'huber']\n",
    "}\n",
    "\n",
    "features = df.drop(['rating'], axis=1)\n",
    "\n",
    "X = features\n",
    "y = df['rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y\n",
    "                                                   , test_size = .3\n",
    "                                                   #, stratify=y\n",
    "                                                   , random_state = 42\n",
    "                                                   , shuffle=True\n",
    "                                                   )\n",
    "\n",
    "\n",
    "CV = GridSearchCV(pl, param_grid, n_jobs= -1)\n",
    "                  \n",
    "CV.fit(X_train, y_train)  \n",
    "print(CV.best_params_)    \n",
    "print(CV.best_score_)\n",
    "\n",
    "# pl.fit(X_train, y_train)\n",
    "# pl.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TEST 1\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total=  10.4s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 7, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 500}\n",
    "# 0.5591003524136465\n",
    "\n",
    "##TEST 2\n",
    "#[Pipeline] ........ (step 2 of 2) Processing classifier, total=   9.6s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 13, 'classifier__min_samples_split': 13, 'classifier__n_estimators': 500}\n",
    "# 0.5611780620229795\n",
    "\n",
    "##TEST 3\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total=   9.5s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 13, 'classifier__min_samples_split': 13, 'classifier__n_estimators': 500}\n",
    "# 0.5615370173848946\n",
    "\n",
    "# [Pipeline] ........ (step 2 of 2) Processing classifier, total=  14.4s\n",
    "# {'classifier__learning_rate': 0.01, 'classifier__max_depth': 6, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 13, 'classifier__min_samples_split': 27, 'classifier__n_estimators': 600}\n",
    "# 0.5625332421263227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(X_train, y_train)\n",
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
